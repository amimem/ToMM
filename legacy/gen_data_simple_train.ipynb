{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a71e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen import generate_data\n",
    "from train import train\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e868e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default args\n",
    "datagen_args={}\n",
    "datagen_args['T'] = 1000\n",
    "datagen_args['corr'] = 1.0\n",
    "datagen_args['A'] = 2\n",
    "datagen_args['N'] = 10\n",
    "datagen_args['M'] = 1\n",
    "datagen_args['K'] = 8\n",
    "datagen_args['num_seeds'] = 2\n",
    "datagen_args['action_selection_method'] = 'greedy'\n",
    "datagen_args['ensemble'] = 'sum'\n",
    "datagen_args['ground_model_name'] = 'bitpop'\n",
    "datagen_args['output'] = 'output/'\n",
    "\n",
    "train_args = {}\n",
    "train_args['model_name']='stomp'\n",
    "train_args['P']=1e6\n",
    "train_args['M']=1\n",
    "train_args['L']=100\n",
    "train_args['n_hidden_layers']=2\n",
    "train_args['n_features']=2\n",
    "train_args['num_codebooks']=10\n",
    "train_args['enc2dec_ratio']=1\n",
    "train_args['epochs']=20\n",
    "train_args['learning_rate']=5e-5\n",
    "train_args['batch_size']=8\n",
    "train_args['outdir']='output/'\n",
    "train_args['data_dir']=''\n",
    "train_args['seed']=0\n",
    "train_args['data_seed']=0\n",
    "train_args['use_lr_scheduler']=False\n",
    "train_args['step_LR']=30\n",
    "train_args['gamma']=0.1\n",
    "train_args['checkpoint_interval']=100\n",
    "train_args['wandb_entity_name']=None\n",
    "train_args['wandb_group_name']=None\n",
    "train_args['wandb_job_type_name']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee4b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable parameters\n",
    "corrvec=[0,1] # [0, 0.5, 1.0]   # agent pairwise action correlation\n",
    "Nvec=[10,100] # [1e1,1e2,1e3,1e4]   # number of agents\n",
    "single_agent_capacity = 256*100\n",
    "Pvec=[Nvec[0]*single_agent_capacity,Nvec[1]*single_agent_capacity]  # number of parameters\n",
    "\n",
    "# datagen vars\n",
    "K=8             # state space dimension\n",
    "M_sys=1         # number of agent groups\n",
    "T=int(1e4)      # number of samples to learn from\n",
    "data_seed = 0   # seed of data generation\n",
    "\n",
    "# train vars\n",
    "M_train=1       # assumed number of agent groups\n",
    "epochs=100\n",
    "\n",
    "datagen_args['K']=K\n",
    "datagen_args['M']=M_sys\n",
    "datagen_args['T']=T\n",
    "datagen_args['ground_model_name'] = 'bitpop'\n",
    "\n",
    "train_args['model_name']='single'\n",
    "train_args['M']=M_train\n",
    "train_args['data_seed']=data_seed\n",
    "train_args['epochs']=epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8052b773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_d1bac3a730\n",
      "bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_4cace0bd77\n",
      "bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_104df71b36\n",
      "bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_ab133d9f57\n"
     ]
    }
   ],
   "source": [
    "# generate data from bitpop\n",
    "hashtype='bitpop_data'\n",
    "hash_data_list=[]\n",
    "for corr in corrvec:\n",
    "    datagen_args['corr']=corr\n",
    "    for N in Nvec:\n",
    "        datagen_args['N']=N\n",
    "        bitpop_data_hash=generate_data(datagen_args.copy())\n",
    "        hash_data_list.append((corr,N,-1,hashtype,bitpop_data_hash))\n",
    "df=pd.DataFrame(hash_data_list,columns=['corr','N','P','hashtype','hash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44966db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Using cpu device\n",
      "seed 0 training of single model with modelsize 256000 for 100 epochs using batchsize 8 and LR 5e-05\n",
      "wandb run name: 7eab1caa5f_20240524-004952\n",
      "using data:output/data_d1bac3a730/data.h5\n",
      "{'file_attrs': {'A': 2, 'K': 8, 'M': 1, 'N': 10, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 0, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': 'd1bac3a730', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004948'}}\n",
      "state_dim: (10000, 8)\n",
      "256000\n",
      "hidden_dim=110\n",
      "number of parameters: 256320\n",
      "gap between P and num_parameters:  -320\n",
      "pre training loss: 0.08657589222431183, acc: 0.5168\n",
      "Epoch 1, loss: 0.080097453, acc: 0.61226\n",
      "Epoch 2, loss: 0.079136306, acc: 0.61265\n",
      "Epoch 3, loss: 0.079112449, acc: 0.61266\n",
      "Epoch 4, loss: 0.079122773, acc: 0.61263\n",
      "Epoch 5, loss: 0.079106439, acc: 0.61245\n",
      "Epoch 6, loss: 0.079114488, acc: 0.61259\n",
      "Epoch 7, loss: 0.079110655, acc: 0.61313\n",
      "Epoch 8, loss: 0.079111715, acc: 0.61167\n",
      "Epoch 9, loss: 0.07909729, acc: 0.61232\n",
      "Epoch 10, loss: 0.079112419, acc: 0.6123\n",
      "Epoch 11, loss: 0.079104571, acc: 0.61233\n",
      "Epoch 12, loss: 0.079110377, acc: 0.61273\n",
      "Epoch 13, loss: 0.079095952, acc: 0.61273\n",
      "Epoch 14, loss: 0.079096073, acc: 0.61221\n",
      "Epoch 15, loss: 0.07910929, acc: 0.61171\n",
      "Epoch 16, loss: 0.079090533, acc: 0.61329\n",
      "Epoch 17, loss: 0.079101435, acc: 0.61278\n",
      "Epoch 18, loss: 0.079085088, acc: 0.6134\n",
      "Epoch 19, loss: 0.079100781, acc: 0.61261\n",
      "Epoch 20, loss: 0.079076089, acc: 0.61202\n",
      "Epoch 21, loss: 0.079095246, acc: 0.61267\n",
      "Epoch 22, loss: 0.07909143, acc: 0.61252\n",
      "Epoch 23, loss: 0.079093298, acc: 0.61276\n",
      "Epoch 24, loss: 0.079087901, acc: 0.61199\n",
      "Epoch 25, loss: 0.079094143, acc: 0.61235\n",
      "Epoch 26, loss: 0.079098354, acc: 0.61231\n",
      "Epoch 27, loss: 0.07908803, acc: 0.61229\n",
      "Epoch 28, loss: 0.079083186, acc: 0.61239\n",
      "Epoch 29, loss: 0.0790824, acc: 0.61265\n",
      "Epoch 30, loss: 0.079097983, acc: 0.61207\n",
      "Epoch 31, loss: 0.079082292, acc: 0.61215\n",
      "Epoch 32, loss: 0.079084493, acc: 0.61257\n",
      "Epoch 33, loss: 0.079099612, acc: 0.61185\n",
      "Epoch 34, loss: 0.07908896, acc: 0.61206\n",
      "Epoch 35, loss: 0.079088335, acc: 0.61181\n",
      "Epoch 36, loss: 0.079083309, acc: 0.61232\n",
      "Epoch 37, loss: 0.079085144, acc: 0.61253\n",
      "Epoch 38, loss: 0.079082798, acc: 0.61265\n",
      "Epoch 39, loss: 0.079083313, acc: 0.61271\n",
      "Epoch 40, loss: 0.079084892, acc: 0.61286\n",
      "Epoch 41, loss: 0.079090033, acc: 0.61251\n",
      "Epoch 42, loss: 0.079093884, acc: 0.61257\n",
      "Epoch 43, loss: 0.079081399, acc: 0.61225\n",
      "Epoch 44, loss: 0.079077268, acc: 0.61246\n",
      "Epoch 45, loss: 0.079084822, acc: 0.61224\n",
      "Epoch 46, loss: 0.079085489, acc: 0.61245\n",
      "Epoch 47, loss: 0.07908696, acc: 0.61232\n",
      "Epoch 48, loss: 0.079087298, acc: 0.61289\n",
      "Epoch 49, loss: 0.079080708, acc: 0.61238\n",
      "Epoch 50, loss: 0.07908901, acc: 0.61233\n",
      "Epoch 51, loss: 0.079082996, acc: 0.61324\n",
      "Epoch 52, loss: 0.079086287, acc: 0.61239\n",
      "Epoch 53, loss: 0.079089112, acc: 0.61191\n",
      "Epoch 54, loss: 0.079080988, acc: 0.61269\n",
      "Epoch 55, loss: 0.07908105, acc: 0.61246\n",
      "Epoch 56, loss: 0.079098173, acc: 0.61237\n",
      "Epoch 57, loss: 0.079086037, acc: 0.61235\n",
      "Epoch 58, loss: 0.079080958, acc: 0.61335\n",
      "Epoch 59, loss: 0.079075949, acc: 0.61292\n",
      "Epoch 60, loss: 0.079082544, acc: 0.61302\n",
      "Epoch 61, loss: 0.079078121, acc: 0.61298\n",
      "Epoch 62, loss: 0.079083221, acc: 0.61318\n",
      "Epoch 63, loss: 0.079090967, acc: 0.61225\n",
      "Epoch 64, loss: 0.079080978, acc: 0.61257\n",
      "Epoch 65, loss: 0.079088827, acc: 0.61202\n",
      "Epoch 66, loss: 0.079078398, acc: 0.61279\n",
      "Epoch 67, loss: 0.079079268, acc: 0.61314\n",
      "Epoch 68, loss: 0.079078105, acc: 0.61266\n",
      "Epoch 69, loss: 0.079094526, acc: 0.61188\n",
      "Epoch 70, loss: 0.079093284, acc: 0.61255\n",
      "Epoch 71, loss: 0.079079616, acc: 0.6126\n",
      "Epoch 72, loss: 0.079087434, acc: 0.61234\n",
      "Epoch 73, loss: 0.079092174, acc: 0.61192\n",
      "Epoch 74, loss: 0.07907582, acc: 0.61336\n",
      "Epoch 75, loss: 0.079088325, acc: 0.61206\n",
      "Epoch 76, loss: 0.079081154, acc: 0.61336\n",
      "Epoch 77, loss: 0.079082475, acc: 0.61208\n",
      "Epoch 78, loss: 0.079090835, acc: 0.6122\n",
      "Epoch 79, loss: 0.079081253, acc: 0.61197\n",
      "Epoch 80, loss: 0.079086713, acc: 0.61245\n",
      "Epoch 81, loss: 0.079080449, acc: 0.61207\n",
      "Epoch 82, loss: 0.079086559, acc: 0.61257\n",
      "Epoch 83, loss: 0.079084117, acc: 0.61252\n",
      "Epoch 84, loss: 0.079088142, acc: 0.61225\n",
      "Epoch 85, loss: 0.079086497, acc: 0.61269\n",
      "Epoch 86, loss: 0.079083581, acc: 0.61286\n",
      "Epoch 87, loss: 0.079081498, acc: 0.6128\n",
      "Epoch 88, loss: 0.079078049, acc: 0.61294\n",
      "Epoch 89, loss: 0.079080667, acc: 0.61215\n",
      "Epoch 90, loss: 0.079076052, acc: 0.61305\n",
      "Epoch 91, loss: 0.079077734, acc: 0.61186\n",
      "Epoch 92, loss: 0.079088434, acc: 0.6124\n",
      "Epoch 93, loss: 0.079089301, acc: 0.61186\n",
      "Epoch 94, loss: 0.079087601, acc: 0.61147\n",
      "Epoch 95, loss: 0.079075687, acc: 0.61334\n",
      "Epoch 96, loss: 0.079079249, acc: 0.61235\n",
      "Epoch 97, loss: 0.079077315, acc: 0.61266\n",
      "Epoch 98, loss: 0.079084368, acc: 0.61349\n",
      "Epoch 99, loss: 0.079080256, acc: 0.61279\n",
      "Epoch 100, loss: 0.079091391, acc: 0.61145\n",
      "~saved model for epoch: 100\n",
      "saving _single_modelsize_256000_trainseed_0_epochs_100_batchsize_8_lr_5e-05 at hash:\n",
      "7eab1caa5f\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Using cpu device\n",
      "seed 0 training of single model with modelsize 2560000 for 100 epochs using batchsize 8 and LR 5e-05\n",
      "wandb run name: 8280dc1fe8_20240524-011052\n",
      "using data:output/data_4cace0bd77/data.h5\n",
      "{'file_attrs': {'A': 2, 'K': 8, 'M': 1, 'N': 100, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 0, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': '4cace0bd77', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004949'}}\n",
      "state_dim: (10000, 8)\n",
      "2560000\n",
      "hidden_dim=110\n",
      "number of parameters: 2563200\n",
      "gap between P and num_parameters:  -3200\n",
      "pre training loss: 0.08667460176849365, acc: 0.500266\n",
      "Epoch 1, loss: 0.080750545, acc: 0.609675\n",
      "Epoch 2, loss: 0.079568724, acc: 0.613853\n",
      "Epoch 3, loss: 0.079562219, acc: 0.613723\n",
      "Epoch 4, loss: 0.079555419, acc: 0.613736\n",
      "Epoch 5, loss: 0.079548449, acc: 0.613881\n",
      "Epoch 6, loss: 0.079537046, acc: 0.613745\n",
      "Epoch 7, loss: 0.079553939, acc: 0.613554\n",
      "Epoch 8, loss: 0.079542534, acc: 0.613766\n",
      "Epoch 9, loss: 0.079545419, acc: 0.613686\n",
      "Epoch 10, loss: 0.079539893, acc: 0.613594\n",
      "Epoch 11, loss: 0.079538423, acc: 0.613839\n",
      "Epoch 12, loss: 0.079542734, acc: 0.613738\n",
      "Epoch 13, loss: 0.079541725, acc: 0.613612\n",
      "Epoch 14, loss: 0.079536165, acc: 0.613675\n",
      "Epoch 15, loss: 0.079538244, acc: 0.613694\n",
      "Epoch 16, loss: 0.079529296, acc: 0.613768\n",
      "Epoch 17, loss: 0.079538975, acc: 0.61365\n",
      "Epoch 18, loss: 0.079531197, acc: 0.613788\n",
      "Epoch 19, loss: 0.079531634, acc: 0.613885\n",
      "Epoch 20, loss: 0.079525504, acc: 0.613814\n",
      "Epoch 21, loss: 0.079525746, acc: 0.613787\n",
      "Epoch 22, loss: 0.07953662, acc: 0.613864\n",
      "Epoch 23, loss: 0.079535064, acc: 0.613777\n",
      "Epoch 24, loss: 0.079527141, acc: 0.614002\n",
      "Epoch 25, loss: 0.079533057, acc: 0.613713\n",
      "Epoch 26, loss: 0.079528547, acc: 0.613892\n",
      "Epoch 27, loss: 0.079535201, acc: 0.613666\n",
      "Epoch 28, loss: 0.079523788, acc: 0.613753\n",
      "Epoch 29, loss: 0.079525922, acc: 0.613731\n",
      "Epoch 30, loss: 0.079530621, acc: 0.613797\n",
      "Epoch 31, loss: 0.079537913, acc: 0.613723\n",
      "Epoch 32, loss: 0.079526109, acc: 0.613748\n",
      "Epoch 33, loss: 0.079532464, acc: 0.613529\n",
      "Epoch 34, loss: 0.079526133, acc: 0.613868\n",
      "Epoch 35, loss: 0.079525956, acc: 0.613833\n",
      "Epoch 36, loss: 0.079533699, acc: 0.613707\n",
      "Epoch 37, loss: 0.079527758, acc: 0.613738\n",
      "Epoch 38, loss: 0.079530935, acc: 0.613744\n",
      "Epoch 39, loss: 0.079526777, acc: 0.613826\n",
      "Epoch 40, loss: 0.079525313, acc: 0.613756\n",
      "Epoch 41, loss: 0.07953359, acc: 0.613813\n",
      "Epoch 42, loss: 0.079528568, acc: 0.613632\n",
      "Epoch 43, loss: 0.079526321, acc: 0.61376\n",
      "Epoch 44, loss: 0.079529571, acc: 0.613676\n",
      "Epoch 45, loss: 0.079529802, acc: 0.613728\n",
      "Epoch 46, loss: 0.079528611, acc: 0.613784\n",
      "Epoch 47, loss: 0.079531209, acc: 0.613685\n",
      "Epoch 48, loss: 0.079526849, acc: 0.613832\n",
      "Epoch 49, loss: 0.079528369, acc: 0.613708\n",
      "Epoch 50, loss: 0.079526481, acc: 0.613896\n",
      "Epoch 51, loss: 0.07952477, acc: 0.613723\n",
      "Epoch 52, loss: 0.079530507, acc: 0.613779\n",
      "Epoch 53, loss: 0.079530249, acc: 0.613722\n",
      "Epoch 54, loss: 0.079524129, acc: 0.61391\n",
      "Epoch 55, loss: 0.079531714, acc: 0.613669\n",
      "Epoch 56, loss: 0.079527022, acc: 0.613733\n",
      "Epoch 57, loss: 0.079523624, acc: 0.613785\n",
      "Epoch 58, loss: 0.07952883, acc: 0.613836\n",
      "Epoch 59, loss: 0.07952971, acc: 0.61377\n",
      "Epoch 60, loss: 0.079528259, acc: 0.613775\n",
      "Epoch 61, loss: 0.079526231, acc: 0.613621\n",
      "Epoch 62, loss: 0.07952932, acc: 0.613753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, loss: 0.079529977, acc: 0.613753\n",
      "Epoch 64, loss: 0.079524267, acc: 0.613707\n",
      "Epoch 65, loss: 0.079522153, acc: 0.613796\n",
      "Epoch 66, loss: 0.079525317, acc: 0.613811\n",
      "Epoch 67, loss: 0.079527138, acc: 0.613831\n",
      "Epoch 68, loss: 0.079525325, acc: 0.613707\n",
      "Epoch 69, loss: 0.079525699, acc: 0.613811\n",
      "Epoch 70, loss: 0.079526095, acc: 0.613827\n",
      "Epoch 71, loss: 0.079528754, acc: 0.613804\n",
      "Epoch 72, loss: 0.079526405, acc: 0.613781\n",
      "Epoch 73, loss: 0.079525793, acc: 0.613809\n",
      "Epoch 74, loss: 0.079526894, acc: 0.6137\n",
      "Epoch 75, loss: 0.079524112, acc: 0.613875\n",
      "Epoch 76, loss: 0.079524824, acc: 0.613759\n",
      "Epoch 77, loss: 0.079528448, acc: 0.613629\n",
      "Epoch 78, loss: 0.079523756, acc: 0.613717\n",
      "Epoch 79, loss: 0.079525323, acc: 0.613747\n",
      "Epoch 80, loss: 0.079526001, acc: 0.613683\n",
      "Epoch 81, loss: 0.079527118, acc: 0.613711\n",
      "Epoch 82, loss: 0.079526941, acc: 0.613693\n",
      "Epoch 83, loss: 0.079523187, acc: 0.613785\n",
      "Epoch 84, loss: 0.079524541, acc: 0.613702\n",
      "Epoch 85, loss: 0.079528241, acc: 0.613775\n",
      "Epoch 86, loss: 0.079526013, acc: 0.613868\n",
      "Epoch 87, loss: 0.07952659, acc: 0.613716\n",
      "Epoch 88, loss: 0.079525582, acc: 0.613794\n",
      "Epoch 89, loss: 0.079527344, acc: 0.61368\n",
      "Epoch 90, loss: 0.079526351, acc: 0.613706\n",
      "Epoch 91, loss: 0.079525385, acc: 0.61373\n",
      "Epoch 92, loss: 0.07952836, acc: 0.613724\n",
      "Epoch 93, loss: 0.079526508, acc: 0.613792\n",
      "Epoch 94, loss: 0.079524544, acc: 0.613729\n",
      "Epoch 95, loss: 0.079525695, acc: 0.613713\n",
      "Epoch 96, loss: 0.079528088, acc: 0.61369\n",
      "Epoch 97, loss: 0.079522128, acc: 0.613837\n",
      "Epoch 98, loss: 0.079527194, acc: 0.613728\n",
      "Epoch 99, loss: 0.079526031, acc: 0.613744\n",
      "Epoch 100, loss: 0.079523305, acc: 0.613761\n",
      "~saved model for epoch: 100\n",
      "saving _single_modelsize_2560000_trainseed_0_epochs_100_batchsize_8_lr_5e-05 at hash:\n",
      "8280dc1fe8\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Using cpu device\n",
      "seed 0 training of single model with modelsize 256000 for 100 epochs using batchsize 8 and LR 5e-05\n",
      "wandb run name: 032be1f813_20240524-044301\n",
      "using data:output/data_104df71b36/data.h5\n",
      "{'file_attrs': {'A': 2, 'K': 8, 'M': 1, 'N': 10, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 1, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': '104df71b36', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004951'}}\n",
      "state_dim: (10000, 8)\n",
      "256000\n",
      "hidden_dim=110\n",
      "number of parameters: 256320\n",
      "gap between P and num_parameters:  -320\n",
      "pre training loss: 0.0866682639503479, acc: 0.49884\n",
      "Epoch 1, loss: 0.052831788, acc: 0.87086\n",
      "Epoch 2, loss: 0.047373641, acc: 0.875\n",
      "Epoch 3, loss: 0.047396132, acc: 0.875\n",
      "Epoch 4, loss: 0.047279773, acc: 0.875\n",
      "Epoch 5, loss: 0.047247694, acc: 0.875\n",
      "Epoch 6, loss: 0.047190391, acc: 0.875\n",
      "Epoch 7, loss: 0.047208707, acc: 0.875\n",
      "Epoch 8, loss: 0.04717644, acc: 0.875\n",
      "Epoch 9, loss: 0.047213013, acc: 0.875\n",
      "Epoch 10, loss: 0.047120242, acc: 0.875\n",
      "Epoch 11, loss: 0.047152916, acc: 0.875\n",
      "Epoch 12, loss: 0.047176532, acc: 0.875\n",
      "Epoch 13, loss: 0.047183734, acc: 0.875\n",
      "Epoch 14, loss: 0.047099271, acc: 0.875\n",
      "Epoch 15, loss: 0.04711583, acc: 0.875\n",
      "Epoch 16, loss: 0.047107249, acc: 0.875\n",
      "Epoch 17, loss: 0.047160902, acc: 0.875\n",
      "Epoch 18, loss: 0.04708603, acc: 0.875\n",
      "Epoch 19, loss: 0.047134091, acc: 0.875\n",
      "Epoch 20, loss: 0.047129151, acc: 0.875\n",
      "Epoch 21, loss: 0.047171764, acc: 0.875\n",
      "Epoch 22, loss: 0.047124757, acc: 0.875\n",
      "Epoch 23, loss: 0.047138759, acc: 0.875\n",
      "Epoch 24, loss: 0.047103672, acc: 0.875\n",
      "Epoch 25, loss: 0.047135622, acc: 0.875\n",
      "Epoch 26, loss: 0.047147813, acc: 0.875\n",
      "Epoch 27, loss: 0.047124007, acc: 0.875\n",
      "Epoch 28, loss: 0.047131913, acc: 0.875\n",
      "Epoch 29, loss: 0.047101218, acc: 0.875\n",
      "Epoch 30, loss: 0.047120293, acc: 0.875\n",
      "Epoch 31, loss: 0.047149718, acc: 0.875\n",
      "Epoch 32, loss: 0.047125753, acc: 0.875\n",
      "Epoch 33, loss: 0.047128701, acc: 0.875\n",
      "Epoch 34, loss: 0.047115395, acc: 0.875\n",
      "Epoch 35, loss: 0.047091142, acc: 0.875\n",
      "Epoch 36, loss: 0.047100608, acc: 0.875\n",
      "Epoch 37, loss: 0.047115422, acc: 0.875\n",
      "Epoch 38, loss: 0.04709889, acc: 0.875\n",
      "Epoch 39, loss: 0.047120824, acc: 0.875\n",
      "Epoch 40, loss: 0.047098076, acc: 0.875\n",
      "Epoch 41, loss: 0.047155714, acc: 0.875\n",
      "Epoch 42, loss: 0.047118678, acc: 0.875\n",
      "Epoch 43, loss: 0.047076441, acc: 0.875\n",
      "Epoch 44, loss: 0.047114328, acc: 0.875\n",
      "Epoch 45, loss: 0.047133569, acc: 0.875\n",
      "Epoch 46, loss: 0.047132999, acc: 0.875\n",
      "Epoch 47, loss: 0.047116554, acc: 0.875\n",
      "Epoch 48, loss: 0.047130514, acc: 0.875\n",
      "Epoch 49, loss: 0.047122398, acc: 0.875\n",
      "Epoch 50, loss: 0.047120375, acc: 0.875\n",
      "Epoch 51, loss: 0.047117526, acc: 0.875\n",
      "Epoch 52, loss: 0.047108808, acc: 0.875\n",
      "Epoch 53, loss: 0.047117535, acc: 0.875\n",
      "Epoch 54, loss: 0.047113526, acc: 0.875\n",
      "Epoch 55, loss: 0.047123332, acc: 0.875\n",
      "Epoch 56, loss: 0.047110499, acc: 0.875\n",
      "Epoch 57, loss: 0.047101953, acc: 0.875\n",
      "Epoch 58, loss: 0.047102658, acc: 0.875\n",
      "Epoch 59, loss: 0.047090948, acc: 0.875\n",
      "Epoch 60, loss: 0.047111031, acc: 0.875\n",
      "Epoch 61, loss: 0.04710155, acc: 0.875\n",
      "Epoch 62, loss: 0.047080259, acc: 0.875\n",
      "Epoch 63, loss: 0.04714733, acc: 0.875\n",
      "Epoch 64, loss: 0.047108333, acc: 0.875\n",
      "Epoch 65, loss: 0.047113583, acc: 0.875\n",
      "Epoch 66, loss: 0.047095177, acc: 0.875\n",
      "Epoch 67, loss: 0.047110981, acc: 0.875\n",
      "Epoch 68, loss: 0.047108134, acc: 0.875\n",
      "Epoch 69, loss: 0.047127789, acc: 0.875\n",
      "Epoch 70, loss: 0.047115463, acc: 0.875\n",
      "Epoch 71, loss: 0.047100106, acc: 0.875\n",
      "Epoch 72, loss: 0.047129196, acc: 0.875\n",
      "Epoch 73, loss: 0.047113558, acc: 0.875\n",
      "Epoch 74, loss: 0.047118936, acc: 0.875\n",
      "Epoch 75, loss: 0.047114809, acc: 0.875\n",
      "Epoch 76, loss: 0.047112739, acc: 0.875\n",
      "Epoch 77, loss: 0.047092051, acc: 0.875\n",
      "Epoch 78, loss: 0.047121589, acc: 0.875\n",
      "Epoch 79, loss: 0.047102993, acc: 0.875\n",
      "Epoch 80, loss: 0.047122136, acc: 0.875\n",
      "Epoch 81, loss: 0.047083747, acc: 0.875\n",
      "Epoch 82, loss: 0.047115734, acc: 0.875\n",
      "Epoch 83, loss: 0.047100076, acc: 0.875\n",
      "Epoch 84, loss: 0.047115314, acc: 0.875\n",
      "Epoch 85, loss: 0.047103189, acc: 0.875\n",
      "Epoch 86, loss: 0.047112922, acc: 0.875\n",
      "Epoch 87, loss: 0.047098747, acc: 0.875\n",
      "Epoch 88, loss: 0.047084619, acc: 0.875\n",
      "Epoch 89, loss: 0.047108067, acc: 0.875\n",
      "Epoch 90, loss: 0.04710246, acc: 0.875\n",
      "Epoch 91, loss: 0.047108523, acc: 0.875\n",
      "Epoch 92, loss: 0.047112064, acc: 0.875\n",
      "Epoch 93, loss: 0.047116845, acc: 0.875\n",
      "Epoch 94, loss: 0.047101568, acc: 0.875\n",
      "Epoch 95, loss: 0.047098335, acc: 0.875\n",
      "Epoch 96, loss: 0.047108364, acc: 0.875\n",
      "Epoch 97, loss: 0.047103939, acc: 0.875\n",
      "Epoch 98, loss: 0.04711429, acc: 0.875\n",
      "Epoch 99, loss: 0.047115828, acc: 0.875\n",
      "Epoch 100, loss: 0.047108363, acc: 0.875\n",
      "~saved model for epoch: 100\n",
      "saving _single_modelsize_256000_trainseed_0_epochs_100_batchsize_8_lr_5e-05 at hash:\n",
      "032be1f813\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Using cpu device\n",
      "seed 0 training of single model with modelsize 2560000 for 100 epochs using batchsize 8 and LR 5e-05\n",
      "wandb run name: d47de5188e_20240524-050336\n",
      "using data:output/data_ab133d9f57/data.h5\n",
      "{'file_attrs': {'A': 2, 'K': 8, 'M': 1, 'N': 100, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 1, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': 'ab133d9f57', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004952'}}\n",
      "state_dim: (10000, 8)\n",
      "2560000\n",
      "hidden_dim=110\n",
      "number of parameters: 2563200\n",
      "gap between P and num_parameters:  -3200\n",
      "pre training loss: 0.08667934648132324, acc: 0.503511\n",
      "Epoch 1, loss: 0.084088946, acc: 0.589546\n",
      "Epoch 2, loss: 0.083661388, acc: 0.594995\n",
      "Epoch 3, loss: 0.083632533, acc: 0.595\n",
      "Epoch 4, loss: 0.083636594, acc: 0.595\n",
      "Epoch 5, loss: 0.08362897, acc: 0.595\n",
      "Epoch 6, loss: 0.08362128, acc: 0.595\n",
      "Epoch 7, loss: 0.083617743, acc: 0.595\n",
      "Epoch 8, loss: 0.08364399, acc: 0.595\n",
      "Epoch 9, loss: 0.083617369, acc: 0.595\n",
      "Epoch 10, loss: 0.083615438, acc: 0.595\n",
      "Epoch 11, loss: 0.083608505, acc: 0.595\n",
      "Epoch 12, loss: 0.083610592, acc: 0.595\n",
      "Epoch 13, loss: 0.083632979, acc: 0.595\n",
      "Epoch 14, loss: 0.083617276, acc: 0.595\n",
      "Epoch 15, loss: 0.083628825, acc: 0.595\n",
      "Epoch 16, loss: 0.083604649, acc: 0.595\n",
      "Epoch 17, loss: 0.083609952, acc: 0.595\n",
      "Epoch 18, loss: 0.083614311, acc: 0.595\n",
      "Epoch 19, loss: 0.083608264, acc: 0.595\n",
      "Epoch 20, loss: 0.08360362, acc: 0.595\n",
      "Epoch 21, loss: 0.083605219, acc: 0.595\n",
      "Epoch 22, loss: 0.08359658, acc: 0.595\n",
      "Epoch 23, loss: 0.083617839, acc: 0.595\n",
      "Epoch 24, loss: 0.083619461, acc: 0.595\n",
      "Epoch 25, loss: 0.083609073, acc: 0.595\n",
      "Epoch 26, loss: 0.083621718, acc: 0.595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, loss: 0.083620149, acc: 0.595\n",
      "Epoch 28, loss: 0.083599263, acc: 0.595\n",
      "Epoch 29, loss: 0.083614097, acc: 0.595\n",
      "Epoch 30, loss: 0.083614538, acc: 0.595\n",
      "Epoch 31, loss: 0.083617145, acc: 0.595\n",
      "Epoch 32, loss: 0.083612831, acc: 0.595\n",
      "Epoch 33, loss: 0.083609752, acc: 0.595\n",
      "Epoch 34, loss: 0.083609127, acc: 0.595\n",
      "Epoch 35, loss: 0.083596817, acc: 0.595\n",
      "Epoch 36, loss: 0.083624121, acc: 0.595\n",
      "Epoch 37, loss: 0.08359702, acc: 0.595\n",
      "Epoch 38, loss: 0.083600846, acc: 0.595\n",
      "Epoch 39, loss: 0.083600626, acc: 0.595\n",
      "Epoch 40, loss: 0.083611497, acc: 0.595\n",
      "Epoch 41, loss: 0.083617362, acc: 0.595\n",
      "Epoch 42, loss: 0.083608773, acc: 0.595\n",
      "Epoch 43, loss: 0.083612049, acc: 0.595\n",
      "Epoch 44, loss: 0.08360888, acc: 0.595\n",
      "Epoch 45, loss: 0.083601897, acc: 0.595\n",
      "Epoch 46, loss: 0.083608047, acc: 0.595\n",
      "Epoch 47, loss: 0.083605326, acc: 0.595\n",
      "Epoch 48, loss: 0.083590352, acc: 0.595\n",
      "Epoch 49, loss: 0.083614824, acc: 0.595\n",
      "Epoch 50, loss: 0.083601117, acc: 0.595\n",
      "Epoch 51, loss: 0.083619516, acc: 0.595\n",
      "Epoch 52, loss: 0.083609537, acc: 0.595\n",
      "Epoch 53, loss: 0.083608865, acc: 0.595\n",
      "Epoch 54, loss: 0.083605773, acc: 0.595\n",
      "Epoch 55, loss: 0.083613504, acc: 0.595\n",
      "Epoch 56, loss: 0.083607192, acc: 0.595\n",
      "Epoch 57, loss: 0.083604727, acc: 0.595\n",
      "Epoch 58, loss: 0.083615846, acc: 0.595\n",
      "Epoch 59, loss: 0.083613008, acc: 0.595\n",
      "Epoch 60, loss: 0.083610336, acc: 0.595\n",
      "Epoch 61, loss: 0.083603595, acc: 0.595\n",
      "Epoch 62, loss: 0.083610746, acc: 0.595\n",
      "Epoch 63, loss: 0.083607131, acc: 0.595\n",
      "Epoch 64, loss: 0.083602853, acc: 0.595\n",
      "Epoch 65, loss: 0.083598194, acc: 0.595\n",
      "Epoch 66, loss: 0.083600601, acc: 0.595\n",
      "Epoch 67, loss: 0.083610321, acc: 0.595\n",
      "Epoch 68, loss: 0.083597547, acc: 0.595\n",
      "Epoch 69, loss: 0.083613194, acc: 0.595\n",
      "Epoch 70, loss: 0.083605641, acc: 0.595\n",
      "Epoch 71, loss: 0.083607688, acc: 0.595\n",
      "Epoch 72, loss: 0.0836034, acc: 0.595\n",
      "Epoch 73, loss: 0.083605596, acc: 0.595\n",
      "Epoch 74, loss: 0.083604956, acc: 0.595\n",
      "Epoch 75, loss: 0.083611578, acc: 0.595\n",
      "Epoch 76, loss: 0.083598084, acc: 0.595\n",
      "Epoch 77, loss: 0.083612697, acc: 0.595\n",
      "Epoch 78, loss: 0.083603558, acc: 0.595\n",
      "Epoch 79, loss: 0.08360658, acc: 0.595\n",
      "Epoch 80, loss: 0.083595629, acc: 0.595\n",
      "Epoch 81, loss: 0.083615359, acc: 0.595\n",
      "Epoch 82, loss: 0.083610775, acc: 0.595\n",
      "Epoch 83, loss: 0.083595844, acc: 0.595\n",
      "Epoch 84, loss: 0.083606884, acc: 0.595\n",
      "Epoch 85, loss: 0.083608211, acc: 0.595\n",
      "Epoch 86, loss: 0.083609558, acc: 0.595\n",
      "Epoch 87, loss: 0.083604471, acc: 0.595\n",
      "Epoch 88, loss: 0.083606111, acc: 0.595\n",
      "Epoch 89, loss: 0.083610191, acc: 0.595\n",
      "Epoch 90, loss: 0.08361058, acc: 0.595\n",
      "Epoch 91, loss: 0.083611373, acc: 0.595\n",
      "Epoch 92, loss: 0.083605262, acc: 0.595\n",
      "Epoch 93, loss: 0.083602478, acc: 0.595\n",
      "Epoch 94, loss: 0.083601713, acc: 0.595\n",
      "Epoch 95, loss: 0.083612347, acc: 0.595\n",
      "Epoch 96, loss: 0.083609736, acc: 0.595\n",
      "Epoch 97, loss: 0.083604975, acc: 0.595\n",
      "Epoch 98, loss: 0.083611133, acc: 0.595\n",
      "Epoch 99, loss: 0.083594505, acc: 0.595\n",
      "Epoch 100, loss: 0.083606918, acc: 0.595\n",
      "~saved model for epoch: 100\n",
      "saving _single_modelsize_2560000_trainseed_0_epochs_100_batchsize_8_lr_5e-05 at hash:\n",
      "d47de5188e\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# train simple\n",
    "hashtype='train_simple'\n",
    "hash_data_list=[]\n",
    "for corr in corrvec:\n",
    "    for nit,N in enumerate(Nvec):\n",
    "        train_args['data_dir']='data_' + df.loc[\n",
    "            (df['corr']==corr) & (df['N']==N) & (df['hashtype']=='bitpop_data'),'hash'].values[0]\n",
    "        train_args['P']=Pvec[nit]\n",
    "        print(type(train_args['P']))\n",
    "        train_simple_hash=train(train_args.copy())\n",
    "        hash_data_list.append((corr,N,Pvec[nit],hashtype,train_simple_hash))\n",
    "dftmp=pd.DataFrame(hash_data_list,columns=['corr','N','P','hashtype','hash'])\n",
    "df=pd.concat((df,dftmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7899d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1bac3a730/7eab1caa5f\n",
      "data_settings:\n",
      "{'A': 2, 'K': 8, 'M': 1, 'N': 10, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 0, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': 'd1bac3a730', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004948'}\n",
      "orig_training_args:\n",
      "{'L': 100, 'M': 1, 'P': 256000, 'batch_size': 8, 'checkpoint_interval': 100, 'data_dir': 'data_d1bac3a730', 'data_seed': 0, 'enc2dec_ratio': 1, 'epochs': 100, 'gamma': 0.1, 'hidden_dim': 110, 'learning_rate': 5e-05, 'model_name': 'single', 'n_features': 2, 'n_hidden_layers': 2, 'num_codebooks': 10, 'outdir': 'output/', 'seed': 0, 'step_LR': 30, 'use_lr_scheduler': False, 'wandb_entity_name': None, 'wandb_group_name': None, 'wandb_job_type_name': None}\n",
      "parameters overwritten from single model at bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_ec6e64862c\n",
      "4cace0bd77/8280dc1fe8\n",
      "data_settings:\n",
      "{'A': 2, 'K': 8, 'M': 1, 'N': 100, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 0, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': '4cace0bd77', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004949'}\n",
      "orig_training_args:\n",
      "{'L': 100, 'M': 1, 'P': 2560000, 'batch_size': 8, 'checkpoint_interval': 100, 'data_dir': 'data_4cace0bd77', 'data_seed': 0, 'enc2dec_ratio': 1, 'epochs': 100, 'gamma': 0.1, 'hidden_dim': 110, 'learning_rate': 5e-05, 'model_name': 'single', 'n_features': 2, 'n_hidden_layers': 2, 'num_codebooks': 10, 'outdir': 'output/', 'seed': 0, 'step_LR': 30, 'use_lr_scheduler': False, 'wandb_entity_name': None, 'wandb_group_name': None, 'wandb_job_type_name': None}\n",
      "parameters overwritten from single model at bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_16ad0eabc1\n",
      "104df71b36/032be1f813\n",
      "data_settings:\n",
      "{'A': 2, 'K': 8, 'M': 1, 'N': 10, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 1, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': '104df71b36', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004951'}\n",
      "orig_training_args:\n",
      "{'L': 100, 'M': 1, 'P': 256000, 'batch_size': 8, 'checkpoint_interval': 100, 'data_dir': 'data_104df71b36', 'data_seed': 0, 'enc2dec_ratio': 1, 'epochs': 100, 'gamma': 0.1, 'hidden_dim': 110, 'learning_rate': 5e-05, 'model_name': 'single', 'n_features': 2, 'n_hidden_layers': 2, 'num_codebooks': 10, 'outdir': 'output/', 'seed': 0, 'step_LR': 30, 'use_lr_scheduler': False, 'wandb_entity_name': None, 'wandb_group_name': None, 'wandb_job_type_name': None}\n",
      "parameters overwritten from single model at bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_25485d8a6b\n",
      "ab133d9f57/d47de5188e\n",
      "data_settings:\n",
      "{'A': 2, 'K': 8, 'M': 1, 'N': 100, 'T': 10000, 'action_selection_method': 'greedy', 'corr': 1, 'ensemble': 'sum', 'ground_model_name': 'bitpop', 'hash': 'ab133d9f57', 'num_seeds': 2, 'output': 'output/', 'timestamp': '20240524_004952'}\n",
      "orig_training_args:\n",
      "{'L': 100, 'M': 1, 'P': 2560000, 'batch_size': 8, 'checkpoint_interval': 100, 'data_dir': 'data_ab133d9f57', 'data_seed': 0, 'enc2dec_ratio': 1, 'epochs': 100, 'gamma': 0.1, 'hidden_dim': 110, 'learning_rate': 5e-05, 'model_name': 'single', 'n_features': 2, 'n_hidden_layers': 2, 'num_codebooks': 10, 'outdir': 'output/', 'seed': 0, 'step_LR': 30, 'use_lr_scheduler': False, 'wandb_entity_name': None, 'wandb_group_name': None, 'wandb_job_type_name': None}\n",
      "parameters overwritten from single model at bitpop\n",
      "running seed 0 of 2\n",
      "running seed 1 of 2\n",
      "saving data_01ed0b9a97\n"
     ]
    }
   ],
   "source": [
    "# generate data from trained simple\n",
    "hashtype = 'simple_data'\n",
    "hash_data_list=[]\n",
    "for corr in corrvec:\n",
    "    for nit,N in enumerate(Nvec):\n",
    "        data_hash=df.loc[\n",
    "            (df['corr']==corr) & (df['N']==N) & (df['hashtype']=='bitpop_data'),'hash'].values[0]\n",
    "        train_hash=df.loc[\n",
    "            (df['corr']==corr) & (df['N']==N) & (df['hashtype']=='train_simple'),'hash'].values[0]\n",
    "        datagen_args['ground_model_name'] = data_hash+'/'+train_hash\n",
    "        simple_data_hash=generate_data(datagen_args.copy())\n",
    "        hash_data_list.append((corr,N,Pvec[nit],hashtype,simple_data_hash))\n",
    "dftmp=pd.DataFrame(hash_data_list,columns=['corr','N','P','hashtype','hash'])\n",
    "df=pd.concat((df,dftmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1f11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store\n",
    "data_store={}\n",
    "data_store['datagen_args']=datagen_args\n",
    "data_store['train_args']=train_args\n",
    "data_store['hashes']=df\n",
    "data_filename = f\"hashlist_K_{K}_Msys_{M_sys}_T_{T}_Mtrain_{M_train}_Ep_{epochs}_dataseed_{data_seed}\"\n",
    "np.save(data_filename+\".npy\",data_store)\n",
    "df.to_csv(data_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c37d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corr</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>hashtype</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>bitpop_data</td>\n",
       "      <td>d1bac3a730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>bitpop_data</td>\n",
       "      <td>4cace0bd77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>bitpop_data</td>\n",
       "      <td>104df71b36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1</td>\n",
       "      <td>bitpop_data</td>\n",
       "      <td>ab133d9f57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256000</td>\n",
       "      <td>train_simple</td>\n",
       "      <td>7eab1caa5f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2560000</td>\n",
       "      <td>train_simple</td>\n",
       "      <td>8280dc1fe8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>256000</td>\n",
       "      <td>train_simple</td>\n",
       "      <td>032be1f813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2560000</td>\n",
       "      <td>train_simple</td>\n",
       "      <td>d47de5188e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>256000</td>\n",
       "      <td>simple_data</td>\n",
       "      <td>ec6e64862c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2560000</td>\n",
       "      <td>simple_data</td>\n",
       "      <td>16ad0eabc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>256000</td>\n",
       "      <td>simple_data</td>\n",
       "      <td>25485d8a6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>2560000</td>\n",
       "      <td>simple_data</td>\n",
       "      <td>01ed0b9a97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corr    N        P      hashtype        hash\n",
       "0     0   10       -1   bitpop_data  d1bac3a730\n",
       "1     0  100       -1   bitpop_data  4cace0bd77\n",
       "2     1   10       -1   bitpop_data  104df71b36\n",
       "3     1  100       -1   bitpop_data  ab133d9f57\n",
       "0     0   10   256000  train_simple  7eab1caa5f\n",
       "1     0  100  2560000  train_simple  8280dc1fe8\n",
       "2     1   10   256000  train_simple  032be1f813\n",
       "3     1  100  2560000  train_simple  d47de5188e\n",
       "0     0   10   256000   simple_data  ec6e64862c\n",
       "1     0  100  2560000   simple_data  16ad0eabc1\n",
       "2     1   10   256000   simple_data  25485d8a6b\n",
       "3     1  100  2560000   simple_data  01ed0b9a97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91451e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #train match\n",
    "# train_args['model_name']='match'\n",
    "# match_train_hashes = []\n",
    "# for data_hash in simple_data_hashes:\n",
    "#   train_args['data_dir']='data_'+data_hash\n",
    "#   match_train_hashes.append(train(train_args.copy()))\n",
    "# # write_hashes(match_train_hashes,'match_train')\n",
    "# hashlist_dict['match_train']=match_train_hashes\n",
    "# print(''.join(['\\n']*10))\n",
    "# np.save(hashlist_filename,hashlist_dict)\n",
    "\n",
    "# #train mlp\n",
    "# train_args['model_name']='match'\n",
    "# match_train_hashes = []\n",
    "# for data_hash in simple_data_hashes:\n",
    "#   train_args['data_dir']='data_'+data_hash\n",
    "#   match_train_hashes.append(train(train_args.copy()))\n",
    "# # write_hashes(match_train_hashes,'match_train')\n",
    "# hashlist_dict['match_train']=match_train_hashes\n",
    "# print(''.join(['\\n']*10))\n",
    "# np.save(hashlist_filename,hashlist_dict)\n",
    "\n",
    "# #generate data from trained match\n",
    "# match_data_hashes=[]\n",
    "# for datahash,trainhash in zip(simple_data_hashes,match_train_hashes):\n",
    "#   datagen_args['ground_model_name'] = datahash+'/'+trainhash\n",
    "#   match_data_hashes.append(generate_data(datagen_args.copy()))\n",
    "# write_hashes(match_data_hashes,'match_data')\n",
    "# hashlist_dict['match_data']=match_data_hashes\n",
    "# print(''.join(['\\n']*10))\n",
    "\n",
    "# np.save(hashlist_filename,hashlist_dict)\n",
    "\n",
    "\n",
    "# def write_hashes(hash_list,hash_name,file_name=hashlist_filename):\n",
    "#   with open(file_name,'a') as f:\n",
    "#       f.write(hash_name)\n",
    "#       for ha in hash_list:\n",
    "#           f.write(ha)\n",
    "# write_hashes(bitpop_data_hashes,'bitpop_data')\n",
    "# write_hashes(simple_train_hashes,'simple_train')\n",
    "# write_hashes(simple_data_hashes,'single_data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
