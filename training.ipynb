{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Sampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "# from torchmetrics import Accuracy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\", flush=True)\n",
    "\n",
    "seed: int = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data in the output folder\n",
    "# outdir = \"/Users/ens/repos/marl/output/\"\n",
    "outdir = \"output/\"\n",
    "data = np.load(outdir + \"_trainingdata_groundmodel_exploit_True_numepi10000_K10_L10_M2_N10_T10.npy\", allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['exploit_mode', 'episode_length', 'num_episodes', 'num_seeds', 'sys_parameters', 'times', 'states', 'actions'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"states\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = data[\"states\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = data[\"actions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = states[0].shape[0]\n",
    "n_hidden = 256\n",
    "n_out = actions[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input, n_hidden, n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "net = Net(n_input, n_hidden, n_out)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, states, actions):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.states[idx], self.actions[idx]\n",
    "    \n",
    "dataset = CustomDataset(states, actions)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 16.965213501281738\n",
      "Epoch 2, loss: 16.960260276641847\n",
      "Epoch 3, loss: 16.954902639923095\n",
      "Epoch 4, loss: 16.949554048461913\n",
      "Epoch 5, loss: 16.945407716827393\n",
      "Epoch 6, loss: 16.941262779083253\n",
      "Epoch 7, loss: 16.938545353851318\n",
      "Epoch 8, loss: 16.934526713256837\n",
      "Epoch 9, loss: 16.92930633804321\n",
      "Epoch 10, loss: 16.928303722534178\n",
      "Epoch 11, loss: 16.922564178009033\n",
      "Epoch 12, loss: 16.920001546020508\n",
      "Epoch 13, loss: 16.917760500183107\n",
      "Epoch 14, loss: 16.914194700164796\n",
      "Epoch 15, loss: 16.9124073197937\n",
      "Epoch 16, loss: 16.908393014831542\n",
      "Epoch 17, loss: 16.905182921600343\n",
      "Epoch 18, loss: 16.903006777496337\n",
      "Epoch 19, loss: 16.901918469543457\n",
      "Epoch 20, loss: 16.89907569503784\n",
      "Epoch 21, loss: 16.896977068786622\n",
      "Epoch 22, loss: 16.894476890563965\n",
      "Epoch 23, loss: 16.89324323196411\n",
      "Epoch 24, loss: 16.89026269378662\n",
      "Epoch 25, loss: 16.887078640136718\n",
      "Epoch 26, loss: 16.886630764007567\n",
      "Epoch 27, loss: 16.885871540374755\n",
      "Epoch 28, loss: 16.882175700531008\n",
      "Epoch 29, loss: 16.88114379043579\n",
      "Epoch 30, loss: 16.878711662139892\n",
      "Epoch 31, loss: 16.876554382019044\n",
      "Epoch 32, loss: 16.873860167236327\n",
      "Epoch 33, loss: 16.873016288757324\n",
      "Epoch 34, loss: 16.87204211959839\n",
      "Epoch 35, loss: 16.87137654129028\n",
      "Epoch 36, loss: 16.86778771972656\n",
      "Epoch 37, loss: 16.867722000579835\n",
      "Epoch 38, loss: 16.865403711547852\n",
      "Epoch 39, loss: 16.864731399841308\n",
      "Epoch 40, loss: 16.863485424957275\n",
      "Epoch 41, loss: 16.860556892700195\n",
      "Epoch 42, loss: 16.86090823776245\n",
      "Epoch 43, loss: 16.86070005340576\n",
      "Epoch 44, loss: 16.858549308929444\n",
      "Epoch 45, loss: 16.85740356048584\n",
      "Epoch 46, loss: 16.855544606933595\n",
      "Epoch 47, loss: 16.854749693603516\n",
      "Epoch 48, loss: 16.85338969543457\n",
      "Epoch 49, loss: 16.8538032711792\n",
      "Epoch 50, loss: 16.851516069488525\n",
      "Epoch 51, loss: 16.85141791366577\n",
      "Epoch 52, loss: 16.848812686767577\n",
      "Epoch 53, loss: 16.848897707977294\n",
      "Epoch 54, loss: 16.845706094818116\n",
      "Epoch 55, loss: 16.84561357696533\n",
      "Epoch 56, loss: 16.846418876190185\n",
      "Epoch 57, loss: 16.845852230529786\n",
      "Epoch 58, loss: 16.842253115081785\n",
      "Epoch 59, loss: 16.84215615951538\n",
      "Epoch 60, loss: 16.83996410644531\n",
      "Epoch 61, loss: 16.84258265563965\n",
      "Epoch 62, loss: 16.842179642333985\n",
      "Epoch 63, loss: 16.839413922271728\n",
      "Epoch 64, loss: 16.840232237701414\n",
      "Epoch 65, loss: 16.839218166809083\n",
      "Epoch 66, loss: 16.837968527984618\n",
      "Epoch 67, loss: 16.83731025314331\n",
      "Epoch 68, loss: 16.836350515594482\n",
      "Epoch 69, loss: 16.835977302093507\n",
      "Epoch 70, loss: 16.834998437957765\n",
      "Epoch 71, loss: 16.833411119537352\n",
      "Epoch 72, loss: 16.833006170349122\n",
      "Epoch 73, loss: 16.833749405822754\n",
      "Epoch 74, loss: 16.832472331848145\n",
      "Epoch 75, loss: 16.83253622894287\n",
      "Epoch 76, loss: 16.832540126342774\n",
      "Epoch 77, loss: 16.830502169799804\n",
      "Epoch 78, loss: 16.831679591064454\n",
      "Epoch 79, loss: 16.830907136993407\n",
      "Epoch 80, loss: 16.82883246154785\n",
      "Epoch 81, loss: 16.828689596557616\n",
      "Epoch 82, loss: 16.826853292541504\n",
      "Epoch 83, loss: 16.829217834014894\n",
      "Epoch 84, loss: 16.83063052444458\n",
      "Epoch 85, loss: 16.82483309310913\n",
      "Epoch 86, loss: 16.829311154022218\n",
      "Epoch 87, loss: 16.82848064453125\n",
      "Epoch 88, loss: 16.826173958587646\n",
      "Epoch 89, loss: 16.82426611907959\n",
      "Epoch 90, loss: 16.82267544143677\n",
      "Epoch 91, loss: 16.82611657043457\n",
      "Epoch 92, loss: 16.82287216644287\n",
      "Epoch 93, loss: 16.822041939697264\n",
      "Epoch 94, loss: 16.823000364532472\n",
      "Epoch 95, loss: 16.822443239898682\n",
      "Epoch 96, loss: 16.821460638275145\n",
      "Epoch 97, loss: 16.82047316848755\n",
      "Epoch 98, loss: 16.820468652648927\n",
      "Epoch 99, loss: 16.82041505493164\n",
      "Epoch 100, loss: 16.820218519592284\n",
      "Epoch 101, loss: 16.82271376586914\n",
      "Epoch 102, loss: 16.819969379119872\n",
      "Epoch 103, loss: 16.820548015594483\n",
      "Epoch 104, loss: 16.81865412689209\n",
      "Epoch 105, loss: 16.818776398773192\n",
      "Epoch 106, loss: 16.815440845336916\n",
      "Epoch 107, loss: 16.818870547332764\n",
      "Epoch 108, loss: 16.81815051422119\n",
      "Epoch 109, loss: 16.81448441040039\n",
      "Epoch 110, loss: 16.816159539031982\n",
      "Epoch 111, loss: 16.817745222015382\n",
      "Epoch 112, loss: 16.818338174285888\n",
      "Epoch 113, loss: 16.816851155700682\n",
      "Epoch 114, loss: 16.81600594985962\n",
      "Epoch 115, loss: 16.814333698120116\n",
      "Epoch 116, loss: 16.815185980682372\n",
      "Epoch 117, loss: 16.81351858596802\n",
      "Epoch 118, loss: 16.814609113006593\n",
      "Epoch 119, loss: 16.815178630371094\n",
      "Epoch 120, loss: 16.81476295288086\n",
      "Epoch 121, loss: 16.814296150360107\n",
      "Epoch 122, loss: 16.81196824737549\n",
      "Epoch 123, loss: 16.813924137573242\n",
      "Epoch 124, loss: 16.809823224334718\n",
      "Epoch 125, loss: 16.813732129974365\n",
      "Epoch 126, loss: 16.812828917236327\n",
      "Epoch 127, loss: 16.812608130493164\n",
      "Epoch 128, loss: 16.81035446044922\n",
      "Epoch 129, loss: 16.811311873321532\n",
      "Epoch 130, loss: 16.81208327041626\n",
      "Epoch 131, loss: 16.810581217041015\n",
      "Epoch 132, loss: 16.80873452484131\n",
      "Epoch 133, loss: 16.809008670806886\n",
      "Epoch 134, loss: 16.81073356185913\n",
      "Epoch 135, loss: 16.80964503677368\n",
      "Epoch 136, loss: 16.80932297454834\n",
      "Epoch 137, loss: 16.80886219924927\n",
      "Epoch 138, loss: 16.810561073303223\n",
      "Epoch 139, loss: 16.808888214111327\n",
      "Epoch 140, loss: 16.810492640380858\n",
      "Epoch 141, loss: 16.812013988189697\n",
      "Epoch 142, loss: 16.80902479690552\n",
      "Epoch 143, loss: 16.807021448364257\n",
      "Epoch 144, loss: 16.808264793548584\n",
      "Epoch 145, loss: 16.808401316833496\n",
      "Epoch 146, loss: 16.807911820373537\n",
      "Epoch 147, loss: 16.806132765045167\n",
      "Epoch 148, loss: 16.807512746429442\n",
      "Epoch 149, loss: 16.806908419952393\n",
      "Epoch 150, loss: 16.80944190032959\n",
      "Epoch 151, loss: 16.806649927978516\n",
      "Epoch 152, loss: 16.805769804229737\n",
      "Epoch 153, loss: 16.805700041046144\n",
      "Epoch 154, loss: 16.80740474899292\n",
      "Epoch 155, loss: 16.806385199890137\n",
      "Epoch 156, loss: 16.8067636529541\n",
      "Epoch 157, loss: 16.804092995910644\n",
      "Epoch 158, loss: 16.80456802017212\n",
      "Epoch 159, loss: 16.806324178619384\n",
      "Epoch 160, loss: 16.803754486846923\n",
      "Epoch 161, loss: 16.805298295440675\n",
      "Epoch 162, loss: 16.805413425445558\n",
      "Epoch 163, loss: 16.805955699310303\n",
      "Epoch 164, loss: 16.80471017349243\n",
      "Epoch 165, loss: 16.805714632263182\n",
      "Epoch 166, loss: 16.803732307281493\n",
      "Epoch 167, loss: 16.804101387023927\n",
      "Epoch 168, loss: 16.805882139892578\n",
      "Epoch 169, loss: 16.80421205001831\n",
      "Epoch 170, loss: 16.8015961328125\n",
      "Epoch 171, loss: 16.80617332839966\n",
      "Epoch 172, loss: 16.80232804626465\n",
      "Epoch 173, loss: 16.803010604858397\n",
      "Epoch 174, loss: 16.800318209533692\n",
      "Epoch 175, loss: 16.803418076324462\n",
      "Epoch 176, loss: 16.805230518188477\n",
      "Epoch 177, loss: 16.80184123184204\n",
      "Epoch 178, loss: 16.801221807250975\n",
      "Epoch 179, loss: 16.80435091339111\n",
      "Epoch 180, loss: 16.798984594573973\n",
      "Epoch 181, loss: 16.80352852584839\n",
      "Epoch 182, loss: 16.799236486206055\n",
      "Epoch 183, loss: 16.80038122344971\n",
      "Epoch 184, loss: 16.797373556518554\n",
      "Epoch 185, loss: 16.801149989318848\n",
      "Epoch 186, loss: 16.798560269622804\n",
      "Epoch 187, loss: 16.797721228637695\n",
      "Epoch 188, loss: 16.800936024017332\n",
      "Epoch 189, loss: 16.801433481292726\n",
      "Epoch 190, loss: 16.7982174307251\n",
      "Epoch 191, loss: 16.800371211853026\n",
      "Epoch 192, loss: 16.7987727684021\n",
      "Epoch 193, loss: 16.79804504547119\n",
      "Epoch 194, loss: 16.799910336914063\n",
      "Epoch 195, loss: 16.799708555603026\n",
      "Epoch 196, loss: 16.801771380462647\n",
      "Epoch 197, loss: 16.7995272756958\n",
      "Epoch 198, loss: 16.79871305831909\n",
      "Epoch 199, loss: 16.800358827819824\n",
      "Epoch 200, loss: 16.799756503448485\n",
      "Epoch 201, loss: 16.800358168182374\n",
      "Epoch 202, loss: 16.8006501071167\n",
      "Epoch 203, loss: 16.79969565994263\n",
      "Epoch 204, loss: 16.797259989471435\n",
      "Epoch 205, loss: 16.800601530303954\n",
      "Epoch 206, loss: 16.79848025390625\n",
      "Epoch 207, loss: 16.798488257751465\n",
      "Epoch 208, loss: 16.79717568344116\n",
      "Epoch 209, loss: 16.799109783325196\n",
      "Epoch 210, loss: 16.79995704788208\n",
      "Epoch 211, loss: 16.79631482208252\n",
      "Epoch 212, loss: 16.798854336853026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/adam.py:154\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/adam.py:99\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     97\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[p]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Lazy state initialization\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# note(crcrpar): [special device hosting for step]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    104\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging_loss = [] \n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss/len(train_loader)}\", flush=True)\n",
    "    logging_loss.append(running_loss/len(train_loader))\n",
    "torch.save(net.state_dict(), outdir + \"groundmodel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net(n_input, n_hidden, n_out)\n",
    "# net.load_state_dict(torch.load(outdir + \"groundmodel.pt\"))\n",
    "# net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on arbitrarily large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Irreducible loss by using groundmodel action probabilities on data (could also add this to data generation script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(inputs)\n",
    "loss = criterion(outputs, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
