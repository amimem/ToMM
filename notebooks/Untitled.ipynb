{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e66de2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3948399847.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    from ../data_gen import generate_data\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen import generate_data\n",
    "from train import train\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb6591e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'argparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43margparse\u001b[49m\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata generation parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--T\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      3\u001b[0m                     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--corr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m      5\u001b[0m                     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction correlation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'argparse' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='data generation parameters')\n",
    "parser.add_argument('--T', type=int,\n",
    "                    default=1000, help='episode length')\n",
    "parser.add_argument('--corr', type=float,\n",
    "                    default=1.0, help='action correlation')\n",
    "parser.add_argument('--A', type=int,\n",
    "                    default=2, help='number of actions')\n",
    "parser.add_argument('--N', type=int,\n",
    "                    default=10, help='number of ground agents')\n",
    "parser.add_argument('--M', type=int,\n",
    "                    default=2, help='number of abstract agents')\n",
    "parser.add_argument('--K', type=int,\n",
    "                    default=3, help='state space dimension')\n",
    "parser.add_argument('--num_seeds', type=int,\n",
    "                    default=10, help='number of seeds')\n",
    "parser.add_argument('--action_selection_method', type=str,\n",
    "                    default='greedy', help='action selection method')\n",
    "parser.add_argument('--ensemble', type=str,\n",
    "                    default='sum', help='ensemble method (sum or mix)')\n",
    "parser.add_argument('--ground_model_name', type=str,\n",
    "                    default='bitpop', help='ground model name or hashes for loaded model')\n",
    "parser.add_argument('--output', type=str,\n",
    "                    default='output/', help='output directory')\n",
    "args = parser.parse_args()\n",
    "\n",
    "#default args\n",
    "datagen_args=vars(args)\n",
    "train_args = {}\n",
    "train_args['model_name']='stomp'\n",
    "train_args['P']=1e6\n",
    "train_args['M']=2\n",
    "train_args['L']=100\n",
    "train_args['n_hidden_layers']=2\n",
    "train_args['n_features']=2\n",
    "train_args['num_codebooks']=10\n",
    "train_args['enc2dec_ratio']=1\n",
    "train_args['epochs']=20\n",
    "train_args['learning_rate']=5e-5\n",
    "train_args['batch_size']=8\n",
    "train_args['outdir']='output/'\n",
    "train_args['data_dir']=''\n",
    "train_args['seed']=0\n",
    "train_args['data_seed']=0\n",
    "train_args['use_lr_scheduler']=False\n",
    "train_args['step_LR']=30\n",
    "train_args['gamma']=0.1\n",
    "train_args['checkpoint_interval']=100\n",
    "train_args['wandb_entity_name']=None\n",
    "train_args['wandb_group_name']=None\n",
    "train_args['wandb_job_type_name']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable parameters\n",
    "corrvec=[0,1] # [0, 0.5, 1.0]   # agent pairwise action correlation\n",
    "Nvec=[10,100] # [1e1,1e2,1e3,1e4]   # number of agents\n",
    "# Pvec=[1e5,1e6] # [1e4,1e5,1e6,1e7]  # number of parameters\n",
    "\n",
    "# datagen vars\n",
    "K=8             # state space dimension\n",
    "M_sys=1         # number of agent groups\n",
    "T=int(1e4)      # number of samples to learn from\n",
    "data_seed = 0   # seed of data generation\n",
    "single_agent_capcity = 256*100\n",
    "# train vars\n",
    "M_train=1       # assumed number of agent groups\n",
    "epochs=1\n",
    "\n",
    "datagen_args['K']=K\n",
    "datagen_args['M']=M_sys\n",
    "datagen_args['T']=T\n",
    "datagen_args['ground_model_name'] = 'bitpop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32368d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data from bitpop\n",
    "hashtype='bitpop_data'\n",
    "hash_data_list=[]\n",
    "for corr in corrvec:\n",
    "    datagen_args['corr']=corr\n",
    "    for N in Nvec:\n",
    "        datagen_args['N']=N\n",
    "        bitpop_data_hash=generate_data(datagen_args.copy())\n",
    "        hash_data_list.append((corr,N,-1,hashtype,bitpop_data_hash))\n",
    "df=pd.DataFrame(hash_data_list,columns=['corr','N','P','hashtype','hash'])\n",
    "train_args['model_name']='single'\n",
    "train_args['M']=M_train\n",
    "train_args['data_seed']=data_seed\n",
    "train_args['epochs']=epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train simple\n",
    "hashtype='train_simple'\n",
    "hash_data_list=[]\n",
    "for corr in corrvec:\n",
    "    for N in Nvec:\n",
    "        train_args['data_dir']='data_' + df.loc[\n",
    "            (df['corr']==corr) & (df['N']==N) & (df['hashtype']=='bitpop_data'),'hash'].values[0]\n",
    "        for P in Pvec:\n",
    "            train_args['P']=N*single_agent_capacity\n",
    "            train_simple_hash=train(train_args.copy())\n",
    "            hash_data_list.append((corr,N,P,hashtype,bitpop_data_hash))\n",
    "dftmp=pd.DataFrame(hash_data_list,columns=['corr','N','P','hashtype','hash'])\n",
    "df=pd.concat((df,dftmp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data from trained simple\n",
    "hashtype = 'simple_data'\n",
    "hash_data_list=[]\n",
    "for corr in corrvec:\n",
    "    for N in Nvec:\n",
    "        data_hash=df.loc[\n",
    "            (df['corr']==corr) & (df['N']==N) & (df['hashtype']=='bitpop_data'),'hash'].values[0]\n",
    "        for P in Pvec:\n",
    "            train_hash=df.loc[\n",
    "                (df['corr']==corr) & (df['N']==N) & (df['hashtype']=='train_simple') & (df['P']==P),'hash'].values\n",
    "            datagen_args['ground_model_name'] = data_hash+'/'+train_hash\n",
    "            simple_data_hash=generate_data(datagen_args.copy())\n",
    "            hash_data_list.append((corr,N,P,hashtype,bitpop_data_hash))\n",
    "dftmp=pd.DataFrame(hash_data_list,columns=['corr','N','P','hashtype','hash'])\n",
    "df=pd.concat((df,dftmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store\n",
    "data_store={}\n",
    "data_store['datagen_args']=datagen_args\n",
    "data_store['train_args']=train_args\n",
    "data_store['hashes']=df\n",
    "data_filename = f\"hashlist_K_{K}_Msys_{M_sys}_T_{T}_Mtrain_{M_train}_Ep_{epochs}_dataseed_{data_seed}\"\n",
    "np.save(data_filename+\".npy\",data_store)\n",
    "df.to_csv(data_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982cfa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #train match\n",
    "# train_args['model_name']='match'\n",
    "# match_train_hashes = []\n",
    "# for data_hash in simple_data_hashes:\n",
    "#   train_args['data_dir']='data_'+data_hash\n",
    "#   match_train_hashes.append(train(train_args.copy()))\n",
    "# # write_hashes(match_train_hashes,'match_train')\n",
    "# hashlist_dict['match_train']=match_train_hashes\n",
    "# print(''.join(['\\n']*10))\n",
    "# np.save(hashlist_filename,hashlist_dict)\n",
    "\n",
    "# #train mlp\n",
    "# train_args['model_name']='match'\n",
    "# match_train_hashes = []\n",
    "# for data_hash in simple_data_hashes:\n",
    "#   train_args['data_dir']='data_'+data_hash\n",
    "#   match_train_hashes.append(train(train_args.copy()))\n",
    "# # write_hashes(match_train_hashes,'match_train')\n",
    "# hashlist_dict['match_train']=match_train_hashes\n",
    "# print(''.join(['\\n']*10))\n",
    "# np.save(hashlist_filename,hashlist_dict)\n",
    "\n",
    "# #generate data from trained match\n",
    "# match_data_hashes=[]\n",
    "# for datahash,trainhash in zip(simple_data_hashes,match_train_hashes):\n",
    "#   datagen_args['ground_model_name'] = datahash+'/'+trainhash\n",
    "#   match_data_hashes.append(generate_data(datagen_args.copy()))\n",
    "# write_hashes(match_data_hashes,'match_data')\n",
    "# hashlist_dict['match_data']=match_data_hashes\n",
    "# print(''.join(['\\n']*10))\n",
    "\n",
    "# np.save(hashlist_filename,hashlist_dict)\n",
    "\n",
    "\n",
    "# def write_hashes(hash_list,hash_name,file_name=hashlist_filename):\n",
    "#   with open(file_name,'a') as f:\n",
    "#       f.write(hash_name)\n",
    "#       for ha in hash_list:\n",
    "#           f.write(ha)\n",
    "# write_hashes(bitpop_data_hashes,'bitpop_data')\n",
    "# write_hashes(simple_train_hashes,'simple_train')\n",
    "# write_hashes(simple_data_hashes,'single_data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
